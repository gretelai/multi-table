{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf78838",
   "metadata": {
    "id": "2bf78838"
   },
   "source": [
    "# Transforms and Multi-Table Relational Databases\n",
    "* This notebook shows how to run transforms directly on a mutli-table relational database \n",
    "* This notebook is discussed in ths [blog](https://gretel.ai/blog/transforms-and-multi-table-relational-databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215d428",
   "metadata": {},
   "source": [
    "## Capabilities\n",
    "* This notebook can be run on any database SQLAlchemy supports such as Postgresql, SQLite or MySQL\n",
    "* This notebook also contains instructions on how to transform data when the relational tables exist in CSV files.\n",
    "* Referential integriety of primary and foreign keys will remain intact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b36e15",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "* The primary and foreign keys in your database must be IDs\n",
    "* Keys cannot be composite keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd52e4",
   "metadata": {},
   "source": [
    "## How to use this notebook on your own dataset\n",
    "* Change the database connection string to refer to your database\n",
    "* Alternatively, change the name and location of the CSV files where your data resides\n",
    "* When viewing your data, change the table names used to your own table names\n",
    "* Modify the location where you'd like your final synthetic data to be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd173c",
   "metadata": {
    "id": "f1cd173c"
   },
   "source": [
    "## Our ecommerce database\n",
    "* Execute the below cell to see a diagram of the database we'll be using in this blueprint. The lines in the diagram show connections between primary and foreign keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152cbcb",
   "metadata": {
    "id": "c152cbcb",
    "outputId": "8154b9a0-2dc8-403d-e986-d98b50c1af82"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"https://gretel-blueprints-pub.s3.us-west-2.amazonaws.com/rdb/ecommerce_db.png\",width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55838e2e",
   "metadata": {
    "id": "55838e2e"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ea4f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a6ea4f8",
    "outputId": "cea46a85-8649-4c33-8f8c-c1af21180df5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!git clone https://github.com/gretelai/multi-table.git\n",
    "\n",
    "os.chdir('./multi-table')\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b11270",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60b11270",
    "outputId": "77c85daa-bb92-4b10-f473-489b0fb4ebac"
   },
   "outputs": [],
   "source": [
    "# Specify your Gretel API key\n",
    "\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "from gretel_client import configure_session, ClientConfig\n",
    "\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "configure_session(ClientConfig(api_key=getpass(prompt=\"Enter Gretel API key\"), \n",
    "                               endpoint=\"https://api.gretel.cloud\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13217ace",
   "metadata": {
    "id": "13217ace"
   },
   "source": [
    "## Gather data and schema relationships directly from a database\n",
    "* For demonstration purposes, we'll first grab our ecommerce SQLite database from S3\n",
    "* This notebook can be run on any database SQLAlchemy supports such as Postgresql or MySQL\n",
    "* For example, if you have a postgres database, simply swap the `sqlite:///` connection string for a `postgres://` one in the `create_engine` command\n",
    "* Using SQLAlchemy's reflection extension, we will crawl the schema, gather table data and produce a list of relationships by table primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870521d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "870521d9",
    "outputId": "2f1380b7-fd2b-4807-b3eb-65fe7bceb8c8"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import multi_table.rdb_util as rdb\n",
    "\n",
    "!wget https://gretel-blueprints-pub.s3.amazonaws.com/rdb/ecom.db\n",
    "    \n",
    "engine = create_engine(\"sqlite:///ecom.db\")\n",
    "\n",
    "rdb_config = rdb.crawl_db(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c3d77",
   "metadata": {
    "id": "d09c3d77"
   },
   "source": [
    "## Alternatively, specify primary/foreign key relationships and locations of data csv files \n",
    "* This is an alternative to the above three cells that work directly with a database\n",
    "* First, assign `base_path` to the directory where the csv files are located.\n",
    "* Then, add a name/key pair for each table name/filename to `rdb_config[\"table_files\"]`\n",
    "* Add all primary keys for each table to `rdb_config[\"primary_keys\"]`\n",
    "* Add all foreign key/primary keys that connect to the same set under `rdb_config[\"relationshipts\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1fea42",
   "metadata": {
    "id": "6b1fea42"
   },
   "outputs": [],
   "source": [
    "# base_path is the directory where your csv files can be found\n",
    "base_path = \"https://gretel-blueprints-pub.s3.amazonaws.com/rdb/\"\n",
    "\n",
    "rdb_config = {\n",
    "   \"table_files\": {\n",
    "      \"users\": base_path + \"users.csv\",\n",
    "\n",
    "      \"order_items\": base_path + \"order_items.csv\",\n",
    "       \n",
    "      \"events\": base_path + \"events.csv\",\n",
    "       \n",
    "      \"inventory_items\": base_path + \"inventory_items.csv\",  \n",
    "       \n",
    "      \"products\": base_path + \"products.csv\",\n",
    "       \n",
    "      \"distribution_center\": base_path + \"distribution_center.csv\"\n",
    "   },\n",
    "    \n",
    "  # List the primary keys for each table\n",
    "    \n",
    "   \"primary_keys\": {\n",
    "      \"users\": \"id\",\n",
    "\n",
    "      \"order_items\": \"id\",\n",
    "       \n",
    "      \"events\": \"id\",\n",
    "       \n",
    "      \"inventory_items\": \"id\",  \n",
    "       \n",
    "      \"products\": \"id\",\n",
    "       \n",
    "      \"distribution_center\": \"id\"\n",
    "   },\n",
    "\n",
    "  # List the (table, field) relationships between primary and foreign keys  \n",
    "   \"relationships\": [\n",
    "          [(\"users\",\"id\"),\n",
    "           (\"order_items\",\"user_id\"),\n",
    "           (\"events\",\"user_id\")\n",
    "          ],         \n",
    "       \n",
    "          [(\"inventory_items\",\"id\"),\n",
    "           (\"order_items\",\"inventory_item_id\")  \n",
    "          ],         \n",
    "\n",
    "          [(\"products\",\"id\"),\n",
    "           (\"inventory_items\",\"product_id\")\n",
    "          ],                \n",
    "\n",
    "          [(\"distribution_center\",\"id\"),\n",
    "           (\"products\",\"distribution_center_id\"),\n",
    "           (\"inventory_items\", \"product_distribution_center_id\")\n",
    "          ]             \n",
    "   ]\n",
    "}\n",
    "\n",
    "# Gather the table data using the filenames entered above\n",
    "\n",
    "rdb_config[\"table_data\"] = {}\n",
    "for table in rdb_config[\"table_files\"]:\n",
    "    filename = rdb_config[\"table_files\"][table]\n",
    "    df = pd.read_csv(filename)\n",
    "    rdb_config[\"table_data\"][table] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8e247",
   "metadata": {
    "id": "79b8e247"
   },
   "source": [
    "## Take a look at your data by joining two tables\n",
    "* Note that every record in the table \"order_items\" matches to an entry in the table \"users\"\n",
    "* An \"inner\" join will take the intersection of two tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8236f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "table1 = \"order_items\"\n",
    "table2 = \"users\"\n",
    "table1_key = \"user_id\"\n",
    "table2_key = \"id\"\n",
    "df1 = rdb_config[\"table_data\"][table1]\n",
    "df2 = rdb_config[\"table_data\"][table2]\n",
    "\n",
    "joined_data = df1.join(df2.set_index(table2_key), how='inner', on=table1_key, lsuffix='_order_items', rsuffix='_users')\n",
    "print(\"Number of records in order_items table is \" + str(len(df1)))\n",
    "print(\"Number of records in user table is \" + str(len(df2)))\n",
    "print(\"Number of records in joined data is \" + str(len(joined_data)))\n",
    "\n",
    "joined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57eeb51",
   "metadata": {
    "id": "d57eeb51"
   },
   "source": [
    "## Define your transform policies\n",
    "* Define one policy per table which transforms any PII or sensitive information that could be used to reidentify a user.\n",
    "* You needn't include a transform for any of the primary/foreign key combinations. We'll be handling those seperately in order to maintain referential integrity.\n",
    "* However, if a table contains a primary key that does not match to a foreign key, that field should be included in the transforms.\n",
    "* Note the tables inventory_items, products and distribution center contain only public information so there will be no transformation.\n",
    "* To run this notebook on a different database, simply enter the table names and policy files below. We will assume all policy files are located in the `policy_dir` defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b6d1a",
   "metadata": {
    "id": "ee8b6d1a"
   },
   "outputs": [],
   "source": [
    "policy_dir = \"https://gretel-blueprints-pub.s3.amazonaws.com/rdb/\"\n",
    "\n",
    "transform_policies = {}\n",
    "transform_policies[\"users\"] = policy_dir + \"users_policy.yaml\"\n",
    "transform_policies[\"order_items\"] = policy_dir + \"order_items_policy.yaml\"\n",
    "transform_policies[\"events\"] = policy_dir + \"events_policy.yaml\"\n",
    "transform_policies[\"inventory_items\"] =  None  \n",
    "transform_policies[\"products\"] = None\n",
    "transform_policies[\"distribution_center\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5ef3b",
   "metadata": {
    "id": "25b5ef3b"
   },
   "source": [
    "## Policy detail\n",
    "* Let's take a detailed look at the transforms for the users table.\n",
    "* Within the `rules` section, we define each type of transformation we want, each one beginning with `- name`.\n",
    "* We start by replacing any field classified as a person name or email address with a fake version.\n",
    "* Note, we choose to leave \"city\", \"state\", \"country\" and \"zip\" as is since it's public knowledge that this database is about user ecommerce transactions in Arizona.\n",
    "* We then transform the \"created_at\" timestamp using a random date shift.\n",
    "* And finally, we transform the numeric fields of age, latitude and longitude with a random numeric shift.\n",
    "* Note, we do not transform \"id\" because it is a primary key that matches to a foreign key. We'll take care of that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dda3cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6dda3cc",
    "outputId": "531ff27a-ccf8-4a80-93f5-93c919819d27"
   },
   "outputs": [],
   "source": [
    "from smart_open import open\n",
    "\n",
    "policy_file = transform_policies[\"users\"]\n",
    "yaml_file = open(policy_file, \"r\")\n",
    "policy = yaml_file.read()\n",
    "yaml_file.close()\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cfe4d5",
   "metadata": {},
   "source": [
    "## Create transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multi_table.transform_models as tm\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "\n",
    "# Designate a project\n",
    "project = create_or_get_unique_project(name=\"rdb-transforms\")\n",
    "\n",
    "# Transform your tables\n",
    "transformed_tables, errors = tm.transform_tables(rdb_config, project, transform_policies)\n",
    "\n",
    "# Tranform your primary/foreign keys\n",
    "if errors == False:\n",
    "    transformed_tables = tm.transform_keys(transformed_tables, rdb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee09d76e",
   "metadata": {
    "id": "ee09d76e"
   },
   "source": [
    "## View the transformed content\n",
    "* We'll again join the order_items and users tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46124e67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "46124e67",
    "outputId": "1983e79f-a10d-43d4-f36a-c4104107a5c0"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "table1 = \"order_items\"\n",
    "table2 = \"users\"\n",
    "table1_key = \"user_id\"\n",
    "table2_key = \"id\"\n",
    "df1 = transformed_tables[table1]\n",
    "df2 = transformed_tables[table2]\n",
    "\n",
    "joined_data = df1.join(df2.set_index(table2_key), how='inner', on=table1_key, lsuffix='_order_items', rsuffix='_users')\n",
    "print(\"Number of records in order_items table is \" + str(len(df1)))\n",
    "print(\"Number of records in user table is \" + str(len(df2)))\n",
    "print(\"Number of records in joined data is \" + str(len(joined_data)))\n",
    "\n",
    "joined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xVfDsgJoL1mz",
   "metadata": {
    "id": "xVfDsgJoL1mz"
   },
   "source": [
    "## Save the transformed data back into a database\n",
    "* Here, we're saving the data into an sqlite database called ecom_xf\n",
    "* To save into a postgres database, use type=\"postgres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new data to ecom_xf using the schema in ecom\n",
    "rdb.save_to_rdb(\"ecom\", \"ecom_xf\", transformed_tables, engine, type=\"sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4302410",
   "metadata": {
    "id": "b4302410"
   },
   "source": [
    "## Alterntively, save the transformed content into CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfd622",
   "metadata": {
    "id": "60dfd622"
   },
   "outputs": [],
   "source": [
    "# Change final_dir to be the location where you'd like your csv files saved\n",
    "final_dir = \"./\"\n",
    "for table in transformed_tables:\n",
    "    df = transformed_tables[table]\n",
    "    filename = final_dir + table + '_transform.csv'\n",
    "    df.to_csv(filename, index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RDB_Transforms.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
